{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from models.GANF import GANF\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# files\n",
    "parser.add_argument('--data_dir', type=str, \n",
    "            default='./data', help='Location of datasets.')\n",
    "parser.add_argument('--output_dir', type=str, \n",
    "            default='./checkpoint/model')\n",
    "parser.add_argument('--name',default='traffic')\n",
    "parser.add_argument('--dataset', type=str, default='metr-la')\n",
    "# restore\n",
    "parser.add_argument('--graph', type=str, default='None')\n",
    "parser.add_argument('--model', type=str, default='None')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed to use.')\n",
    "# model parameters\n",
    "parser.add_argument('--n_blocks', type=int, default=6, help='Number of blocks to stack in a model (MADE in MAF; Coupling+BN in RealNVP).')\n",
    "parser.add_argument('--n_components', type=int, default=1, help='Number of Gaussian clusters for mixture of gaussians models.')\n",
    "parser.add_argument('--hidden_size', type=int, default=32, help='Hidden layer size for MADE (and each MADE block in an MAF).')\n",
    "parser.add_argument('--n_hidden', type=int, default=1, help='Number of hidden layers in each MADE.')\n",
    "parser.add_argument('--batch_norm', type=bool, default=False)\n",
    "# training params\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--n_workers', type=int, default=0)\n",
    "parser.add_argument('--data_mode', type=str, default=None, help='Select debug for running with 0.05 data')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4)\n",
    "parser.add_argument('--n_epochs', type=int, default=20)\n",
    "parser.add_argument('--n_refinment_iter', type=int, default=100)\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate.')\n",
    "parser.add_argument('--log_interval', type=int, default=5, help='How often to show loss statistics and save samples.')\n",
    "\n",
    "parser.add_argument('--h_tol', type=float, default=1e-6)\n",
    "parser.add_argument('--rho_max', type=float, default=1e16)\n",
    "parser.add_argument('--max_iter', type=int, default=20)\n",
    "parser.add_argument('--lambda1', type=float, default=0.0)\n",
    "parser.add_argument('--rho_init', type=float, default=1.0)\n",
    "parser.add_argument('--alpha_init', type=float, default=0.0)\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "\n",
    "print(args)\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset\")\n",
    "from dataset import load_traffic\n",
    "\n",
    "train_loader, val_loader, test_loader, n_sensor = load_traffic(\"{}/{}.h5\".format(args.data_dir,args.dataset), \\\n",
    "                                                                args.batch_size, args.n_workers, args.data_mode)\n",
    "#%%\n",
    "\n",
    "rho = args.rho_init\n",
    "alpha = args.alpha_init\n",
    "lambda1 = args.lambda1\n",
    "h_A_old = np.inf\n",
    "\n",
    "\n",
    "max_iter = args.max_iter\n",
    "rho_max = args.rho_max\n",
    "h_tol = args.h_tol\n",
    "epoch = 0\n",
    "\n",
    "# initialize A (adjacency matrix of the graph)\n",
    "if args.graph != 'None':\n",
    "    init = torch.load(args.graph).to(device).abs()\n",
    "    print(\"Load graph from \"+args.graph)\n",
    "else:\n",
    "    from torch.nn.init import xavier_uniform_\n",
    "    init = torch.zeros([n_sensor, n_sensor])\n",
    "    init = xavier_uniform_(init).abs()\n",
    "    init = init.fill_diagonal_(0.0)\n",
    "A = torch.tensor(init, requires_grad=True, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GANF(args.n_blocks, 1, args.hidden_size, args.n_hidden, dropout=0.0, batch_norm=args.batch_norm)\n",
    "model = model.to(device)\n",
    "\n",
    "if args.model != 'None':\n",
    "    model.load_state_dict(torch.load(args.model))\n",
    "    print('Load model from '+args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_value_\n",
    "save_path = os.path.join(args.output_dir,args.name)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "loss_best = 100\n",
    "for _ in range(max_iter):\n",
    "\n",
    "    while rho < rho_max:\n",
    "        lr = args.lr #* np.math.pow(0.1, epoch // 100)\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params':model.parameters(), 'weight_decay':args.weight_decay},\n",
    "            {'params': [A]}], lr=lr, weight_decay=0.0)\n",
    "        # train\n",
    "        \n",
    "        for _ in range(args.n_epochs): # 20 epoche di default\n",
    "            epoch_progress = 0\n",
    "            len_train_loader = len(train_loader)\n",
    "            # train\n",
    "            loss_train = []\n",
    "            epoch += 1\n",
    "            model.train()\n",
    "            for x in train_loader:\n",
    "                epoch_progress += 1/len_train_loader\n",
    "                x = x.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                A_hat = torch.divide(A.T,A.sum(dim=1).detach()).T #normalizzazione della matrice di adiacenza\n",
    "                loss = -model(x, A_hat) #dobbiamo massimizzare la logprob in uscita quindi la loss è -ouput_modello\n",
    "                h = torch.trace(torch.matrix_exp(A_hat*A_hat)) - n_sensor #non so cosa sia, è un elemento che viene aggiunto nella loss. che sia un modo per minimizzare qualche caratteristica della matrice di adiacenza?\n",
    "                total_loss = loss + 0.5 * rho * h * h + alpha * h # di default alpha=0 e rho=1\n",
    "\n",
    "                total_loss.backward()\n",
    "                clip_grad_value_(model.parameters(), 1)\n",
    "                optimizer.step()\n",
    "                loss_train.append(loss.item())\n",
    "                A.data.copy_(torch.clamp(A.data, min=0, max=1)) #clampa tutti i valori di A tra 0 e 1 in maniera brutale (non mi piace)\n",
    "            \n",
    "            # evaluate \n",
    "            model.eval()\n",
    "            loss_val = []\n",
    "            with torch.no_grad():\n",
    "                for x in val_loader: \n",
    "                    # prende tutte le istanze del validation dataset, le passa attraverso il modello e ne valuta la logprob per poi considerarne la media\n",
    "                    x = x.to(device)\n",
    "                    loss = -model(x,A_hat.data)\n",
    "                    loss_val.append(loss.item())\n",
    "            \n",
    "            print('Epoch: {}, train -log_prob: {:.2f}, test -log_prob: {:.2f}, h: {}'\\\n",
    "                    .format(epoch, np.mean(loss_train), np.mean(loss_val), h.item()))\n",
    "\n",
    "            if np.mean(loss_val) < loss_best:\n",
    "                #se la loss è migliorata allora salviamo il modello dopo lo step dell'ottimizzatore (alla fine avremo salvato il modello con la loss migliore)\n",
    "                loss_best = np.mean(loss_val)\n",
    "                print(\"save model {} epoch\".format(epoch))\n",
    "                torch.save(A.data,os.path.join(save_path, \"graph_best.pt\"))\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, \"{}_best.pt\".format(args.name)))\n",
    "        \n",
    "    \n",
    "        print('rho: {}, alpha {}, h {}'.format(rho, alpha, h.item()))\n",
    "        print('===========================================')\n",
    "        torch.save(A.data,os.path.join(save_path, \"graph_{}.pt\".format(epoch)))\n",
    "        torch.save(model.state_dict(), os.path.join(save_path, \"{}_{}.pt\".format(args.name, epoch)))\n",
    "    \n",
    "        del optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if h.item() > 0.5 * h_A_old:\n",
    "            #andiamo a controllare se il valore di h è soddisfacente: se non lo è aumentiamo il valore di rho che va a pesare di più h nella loss\n",
    "            rho *= 10\n",
    "        else:\n",
    "            # se h è abbastanza basso stoppiamo il training\n",
    "            # NB h è molto importante perché controlla lo stop del training\n",
    "            break\n",
    "\n",
    "    h_A_old = h.item()\n",
    "    alpha += rho*h.item()\n",
    "\n",
    "    if h_A_old <= h_tol or rho >=rho_max:\n",
    "        #se siamo già scesi sotto la tol di h stoppiamo il training\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopo aver ottenuto un buon valore di h andiamo ad affinare il learning rate e dare un'altra botta di training (100 outer iteration quindi 100*n_epochs girate di dataset)\n",
    "\n",
    "lr = args.lr * 0.1\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':model.parameters(), 'weight_decay':args.weight_decay},\n",
    "    {'params': [A]}], lr=lr, weight_decay=0.0)\n",
    "# train\n",
    "\n",
    "for _ in range(args.n_refinement_iter):\n",
    "    loss_train = []\n",
    "    epoch += 1\n",
    "    model.train()\n",
    "    for x in train_loader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        A_hat = torch.divide(A.T,A.sum(dim=1).detach()).T\n",
    "        loss = -model(x, A_hat)\n",
    "        h = torch.trace(torch.matrix_exp(A_hat*A_hat)) - n_sensor\n",
    "        total_loss = loss + 0.5 * rho * h * h + alpha * h \n",
    "\n",
    "        total_loss.backward()\n",
    "        clip_grad_value_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        loss_train.append(loss.item())\n",
    "        A.data.copy_(torch.clamp(A.data, min=0, max=1))\n",
    "\n",
    "    model.eval()\n",
    "    loss_val = []\n",
    "    print(A.max())\n",
    "    with torch.no_grad():\n",
    "        for x in val_loader:\n",
    "\n",
    "            x = x.to(device)\n",
    "            loss = -model(x,A_hat.data)\n",
    "            loss_val.append(loss.item())\n",
    "    \n",
    "    print('Epoch: {}, train -log_prob: {:.2f}, test -log_prob: {:.2f}, h: {}'\\\n",
    "            .format(epoch, np.mean(loss_train), np.mean(loss_val), h.item()))\n",
    "\n",
    "    if np.mean(loss_val) < loss_best:\n",
    "        loss_best = np.mean(loss_val)\n",
    "        print(\"save model {} epoch\".format(epoch))\n",
    "        torch.save(A.data,os.path.join(save_path, \"graph_best.pt\"))\n",
    "        torch.save(model.state_dict(), os.path.join(save_path, \"{}_best.pt\".format(args.name)))\n",
    "\n",
    "    if epoch % args.log_interval==0:\n",
    "        torch.save(A.data,os.path.join(save_path, \"graph_{}.pt\".format(epoch)))\n",
    "        torch.save(model.state_dict(), os.path.join(save_path, \"{}_{}.pt\".format(args.name, epoch)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
